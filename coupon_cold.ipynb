{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer,OneHotEncoder,Imputer,StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from scipy import sparse\n",
    "import scipy    \n",
    "import scipy.sparse as sp\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Not all data could be standardNormalized, Nonnegative MF could be not standarlized!!use min-max instead please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StandardScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read five tables\n",
    "df_coupon_train=pd.read_csv('input/coupon_list_train.csv')\n",
    "df_coupon_test=pd.read_csv('input/coupon_list_test.csv')\n",
    "df_coupon_buy=pd.read_csv('input/coupon_detail_train.csv')\n",
    "df_coupon_visit=pd.read_csv('input/coupon_visit_train.csv')\n",
    "df_user=pd.read_csv('input/user_list.csv')\n",
    "df_sub=pd.read_csv('input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22863</th>\n",
       "      <th>22864</th>\n",
       "      <th>22865</th>\n",
       "      <th>22866</th>\n",
       "      <th>22867</th>\n",
       "      <th>22868</th>\n",
       "      <th>22869</th>\n",
       "      <th>22870</th>\n",
       "      <th>22871</th>\n",
       "      <th>22872</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USER_ID_hash</th>\n",
       "      <td>0000b53e182165208887ba65c079fc21</td>\n",
       "      <td>00035b86e6884589ec8d28fbf2fe7757</td>\n",
       "      <td>0005b1068d5f2b8f2a7c978fcfe1ca06</td>\n",
       "      <td>000cc06982785a19e2a2fdb40b1c9d59</td>\n",
       "      <td>0013518e41c416cd6a181d277dd8ca0b</td>\n",
       "      <td>001acdee812a18acfd7509172bed5700</td>\n",
       "      <td>001fd7876e3aa29393537c6baf308e43</td>\n",
       "      <td>002383753c1e5d6305c8aff6f89e26d6</td>\n",
       "      <td>0025cae7997d25ea5cf8851bb099c798</td>\n",
       "      <td>002822059a01d895fad84f2f2ff5c1f1</td>\n",
       "      <td>...</td>\n",
       "      <td>ffe65332382f1f264d84fa85a44f564c</td>\n",
       "      <td>ffe7362865b91c287affaf8912ef4750</td>\n",
       "      <td>ffecfec48a5778f3e470d8a4fb804723</td>\n",
       "      <td>ffed1129803b26815be16d5531a27250</td>\n",
       "      <td>fff193f946ae515d405376cdf7895f78</td>\n",
       "      <td>fff1a623187cefd7a594e338709b0f40</td>\n",
       "      <td>fff4a076cfda6ff9dbe85e1cb678791b</td>\n",
       "      <td>fff970d2014c3e10a77e38d540239017</td>\n",
       "      <td>fffafc024e264d5d539813444cf61199</td>\n",
       "      <td>ffff56dbf3c782c3532f88c6c79817ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASED_COUPONS</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22873 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              0      \\\n",
       "USER_ID_hash       0000b53e182165208887ba65c079fc21   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              1      \\\n",
       "USER_ID_hash       00035b86e6884589ec8d28fbf2fe7757   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              2      \\\n",
       "USER_ID_hash       0005b1068d5f2b8f2a7c978fcfe1ca06   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              3      \\\n",
       "USER_ID_hash       000cc06982785a19e2a2fdb40b1c9d59   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              4      \\\n",
       "USER_ID_hash       0013518e41c416cd6a181d277dd8ca0b   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              5      \\\n",
       "USER_ID_hash       001acdee812a18acfd7509172bed5700   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              6      \\\n",
       "USER_ID_hash       001fd7876e3aa29393537c6baf308e43   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              7      \\\n",
       "USER_ID_hash       002383753c1e5d6305c8aff6f89e26d6   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              8      \\\n",
       "USER_ID_hash       0025cae7997d25ea5cf8851bb099c798   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              9      \\\n",
       "USER_ID_hash       002822059a01d895fad84f2f2ff5c1f1   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                 ...                 \\\n",
       "USER_ID_hash                     ...                  \n",
       "PURCHASED_COUPONS                ...                  \n",
       "\n",
       "                                              22863  \\\n",
       "USER_ID_hash       ffe65332382f1f264d84fa85a44f564c   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              22864  \\\n",
       "USER_ID_hash       ffe7362865b91c287affaf8912ef4750   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              22865  \\\n",
       "USER_ID_hash       ffecfec48a5778f3e470d8a4fb804723   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              22866  \\\n",
       "USER_ID_hash       ffed1129803b26815be16d5531a27250   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              22867  \\\n",
       "USER_ID_hash       fff193f946ae515d405376cdf7895f78   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              22868  \\\n",
       "USER_ID_hash       fff1a623187cefd7a594e338709b0f40   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              22869  \\\n",
       "USER_ID_hash       fff4a076cfda6ff9dbe85e1cb678791b   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              22870  \\\n",
       "USER_ID_hash       fff970d2014c3e10a77e38d540239017   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              22871  \\\n",
       "USER_ID_hash       fffafc024e264d5d539813444cf61199   \n",
       "PURCHASED_COUPONS                               NaN   \n",
       "\n",
       "                                              22872  \n",
       "USER_ID_hash       ffff56dbf3c782c3532f88c6c79817ba  \n",
       "PURCHASED_COUPONS                               NaN  \n",
       "\n",
       "[2 rows x 22873 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19413, 22873), (32628, 22805), (19368, 22782))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#analysis for data inconsistency\n",
    "item_num_total,user_num_total=len(df_coupon_train.COUPON_ID_hash),len(df_user.USER_ID_hash)#dont' user unique uid_rating, otherwise the index will exceed\n",
    "item_num_visit=len(df_coupon_visit.VIEW_COUPON_ID_hash.unique())\n",
    "user_num_visit=len(df_coupon_visit.USER_ID_hash.unique())\n",
    "item_num_buy=len(df_coupon_buy.COUPON_ID_hash.unique())\n",
    "user_num_buy=len(df_coupon_buy.USER_ID_hash.unique())\n",
    "(item_num_total,user_num_total),(item_num_visit,user_num_visit),(item_num_buy,user_num_buy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Genearate the leakage user rating from bestSubmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user intesect user_visit is 22805\n",
      "user intesect user_buy is 22782\n",
      "user num 22873\n",
      "True\n",
      "critical user num 40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "set_coupon_train=set(df_coupon_train.COUPON_ID_hash.unique())\n",
    "set_coupon_buy=set(df_coupon_buy.COUPON_ID_hash.unique())\n",
    "set_coupon_visit=set(df_coupon_visit.VIEW_COUPON_ID_hash.unique())\n",
    "set_coupon_test=set(df_coupon_test.COUPON_ID_hash.unique())\n",
    "set_user_buy=set(df_coupon_buy.USER_ID_hash.unique())\n",
    "set_user=set(df_user.USER_ID_hash.unique())\n",
    "set_user_visit=set(df_coupon_visit.USER_ID_hash.unique())\n",
    "print 'user intesect user_visit is %s' %len(set_user.intersection(set_user_visit))\n",
    "print 'user intesect user_buy is %s' %len(set_user.intersection(set_user_buy))\n",
    "print 'user num %s' %len(set_user)\n",
    "print (set_user_visit.union(set_user_buy)).issubset(set_user)\n",
    "print 'critical user num %s' %len(set_user.difference(set_user_visit.union(set_user_buy)))\n",
    "set_user_leakage=set_user.difference(set_user_buy)#these users are not included in training\n",
    "series_user_leakage=pd.Series(list(set_user_leakage))\n",
    "df_others_submission=pd.read_csv('bestSubmission.csv')\n",
    "df_leakage=df_others_submission.set_index('USER_ID_hash').loc[series_user_leakage].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "coupon_train and coupon_visit intersect by 19412\n",
      "coupon_train and coupon_buy intersect by 19368\n",
      "coupon_test and coupon_buy intersect by 0\n",
      "coupon_test and coupon_visit intersect by 39\n",
      "#coupon_train and coupon_test intersect by 0\n",
      "#coupon_visit not in coupon_train 13216\n"
     ]
    }
   ],
   "source": [
    "#df_coupon_buy is contained in set_coupon_train\n",
    "print set_coupon_train.issuperset(set_coupon_buy)\n",
    "print set_coupon_train.issubset(set_coupon_visit)\n",
    "print 'coupon_train and coupon_visit intersect by %s'%len(set_coupon_train.intersection(set_coupon_visit))\n",
    "print 'coupon_train and coupon_buy intersect by %s' %len(set_coupon_train.intersection(set_coupon_buy))\n",
    "print 'coupon_test and coupon_buy intersect by %s' %len(set_coupon_test.intersection(set_coupon_buy))\n",
    "print 'coupon_test and coupon_visit intersect by %s'%len(set_coupon_test.intersection(set_coupon_visit))\n",
    "print '#coupon_train and coupon_test intersect by %s'%len(set_coupon_train.intersection(set_coupon_test))\n",
    "print '#coupon_visit not in coupon_train %s'%len(set_coupon_visit.difference(set_coupon_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_user_index=df_user.index # all the users now has a unique index\n",
    "#part_coupon_index_train=df_coupon_train.index # all the coupons in training set now has a unique index\n",
    "#uid_rating=df_user.set_index('USER_ID_hash').loc[df_rating['USER_ID_hash']]['user_global_index'].values\n",
    "#qid_rating=df_coupon_train.set_index('COUPON_ID_hash').loc[df_rating['COUPON_ID_hash']]['coupon_global_index'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generate MAP from coupon_id to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_MAP_user=pd.DataFrame(df_coupon_buy.USER_ID_hash.unique(),columns=['USER_ID_hash'])\n",
    "df_MAP_user['user_idx']=df_MAP_user.index\n",
    "df_MAP_coupon=pd.DataFrame(df_coupon_buy.COUPON_ID_hash.unique(),columns=['COUPON_ID_hash'])\n",
    "df_MAP_coupon['coupon_idx']=df_MAP_coupon.index\n",
    "\n",
    "df_testsetMAP_Coupon=pd.DataFrame(df_coupon_test.COUPON_ID_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generate Xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_rating=df_coupon_buy[[u'USER_ID_hash',u'COUPON_ID_hash','ITEM_COUNT']]\n",
    "df_rating['ITEM_COUNT'].apply(lambda x : 1 if x >0 else x)\n",
    "\n",
    "#WITH merge, one could generate repetitive index for coupon and user to have sparse matrix\n",
    "df_rating=df_rating.merge(df_MAP_user,on='USER_ID_hash',how='left').merge(df_MAP_coupon,on='COUPON_ID_hash',how='left')\n",
    "\n",
    "V=df_rating.ITEM_COUNT\n",
    "qid_rating_repetitive=df_rating.coupon_idx\n",
    "uid_rating_repetitive=df_rating.user_idx\n",
    "Xu_train=sparse.coo_matrix((V,(qid_rating_repetitive,uid_rating_repetitive)),shape=(len(np.unique(qid_rating_repetitive)),len(np.unique(uid_rating_repetitive)))) # the user_id looks like (1,1,1,2,2,3,4,4,...)\n",
    "#size of item-user should be (19413, 22873),but in df_coupon_buy, the size is only (19368, 22782)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "script_path = os.path.abspath(os.path.dirname('__file__'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def extract_topn_item4user(df_itemUser_groupby_user, n=10, column=\"predict\", merge_column=\"COUPON_ID_hash\"):\n",
    "    '''\n",
    "    get top n row\n",
    "    :param pandas.DataFrame df:\n",
    "    :param int n:\n",
    "    :param str column:\n",
    "    :rtype: pandas.DataFrame\n",
    "    '''\n",
    "    return \" \".join(df_itemUser_groupby_user.sort_index(by=column)[-n:][merge_column])\n",
    "\n",
    "def binarizeFrame(df):\n",
    "    dic=df.T.to_dict().values()\n",
    "    vectorizer = DV( sparse = False )\n",
    "    vec_x_cat_train = vectorizer.fit_transform( dic)\n",
    "    return vec_x_cat_train\n",
    "\n",
    "class Get_Match_Pref(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    get user pref is match coupon area\n",
    "    '''\n",
    "\n",
    "    def get_feature_names(self):\n",
    "\n",
    "        return [self.__class__.__name__]\n",
    "\n",
    "    def fit(self, date_frame, y=None):\n",
    "        '''\n",
    "        fit\n",
    "\n",
    "        :param pandas.DataFrame: all data\n",
    "        :rtype: Get_Price_Rate\n",
    "        '''\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, date_frame):\n",
    "        '''\n",
    "        transform\n",
    "\n",
    "        :param pandas.DataFrame: all data\n",
    "        :rtype: array\n",
    "        '''\n",
    "        res_sr = date_frame[\"PREF_NAME\"] == date_frame[\"ken_name\"]\n",
    "\n",
    "        return res_sr.as_matrix()[None].T.astype(np.float)\n",
    "\n",
    "class Get_user_feature(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "\n",
    "        return [self.__class__.__name__]\n",
    "\n",
    "    def fit(self, data_frame, y=None):\n",
    "        '''\n",
    "        fit\n",
    "        :param pandas.DataFrame: all data\n",
    "        :rtype: Get_Price_Rate\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df_coupon_train):\n",
    "        '''\n",
    "        transform\n",
    "        :param pandas.DataFrame: all data\n",
    "        :rtype: array\n",
    "        '''\n",
    "        le=LabelEncoder()\n",
    "        df_coupon_train['SEX_ID_Num']=le.fit_transform(df_coupon_train['SEX_ID'])\n",
    "        return df_coupon_train[['SEX_ID_Num', 'AGE']].as_matrix().astype(np.float)\n",
    "\n",
    "\n",
    "          \n",
    "class Get_coupon_feature(BaseEstimator, TransformerMixin):\n",
    "    def encode(self,v):\n",
    "        u = v.unique()\n",
    "        lm = len(u) + 1\n",
    "        d = {np.nan:lm, \"nan\": lm}  \n",
    "        for ii, iu in enumerate(u):\n",
    "            d[iu] = ii\n",
    "        return v.apply(lambda x: d.get(x, lm) ).values\n",
    "\n",
    "    def binarizeCategorical(self,df_coupon_cate):\n",
    "        oh=OneHotEncoder()\n",
    "        res_final = None\n",
    "        for i, col in enumerate(df_coupon_cate.columns):\n",
    "            #print col\n",
    "            res = self.encode(df_coupon_cate[col])\n",
    "            #print res.shape, np.unique(res), \n",
    "            #print len(df_coupon_train_cat[col].unique())\n",
    "            res = oh.fit_transform(res.reshape(len(res), 1))\n",
    "            res_final = res if res_final is None else sp.hstack((res_final, res))\n",
    "            #print res_final.shape\n",
    "        return res_final.todense()\n",
    "    \n",
    "    \n",
    "    def get_feature_names(self):\n",
    "\n",
    "        return [self.__class__.__name__]\n",
    "\n",
    "    def fit(self, df_coupon_train_cat, y=None):\n",
    "        '''\n",
    "        fit\n",
    "        :param pandas.DataFrame: all data\n",
    "        :rtype: Get_Price_Rate\n",
    "        '''\n",
    "        #df_coupon_train_cat.apply(lambda x: len(x.unique()))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df_coupon):\n",
    "        '''\n",
    "        transform\n",
    "        :param pandas.DataFrame: coupon property pandas data frame\n",
    "        :rtype: array\n",
    "        '''\n",
    "        #'COUPON_ID_hash',\n",
    "        df_coupon_nume=df_coupon[['PRICE_RATE', 'CATALOG_PRICE','DISCOUNT_PRICE','VALIDPERIOD','DISPPERIOD',\\\n",
    "                                              'USABLE_DATE_MON', 'USABLE_DATE_TUE',\\\n",
    "               'USABLE_DATE_WED', 'USABLE_DATE_THU', 'USABLE_DATE_FRI',\\\n",
    "               'USABLE_DATE_SAT', 'USABLE_DATE_SUN', 'USABLE_DATE_HOLIDAY','USABLE_DATE_BEFORE_HOLIDAY']]\n",
    "        im=Imputer()\n",
    "        mm=MinMaxScaler()\n",
    "        ss=StandardScaler(with_mean=False)\n",
    "        df_coupon_nume_im=pd.DataFrame(im.fit_transform(df_coupon_nume),columns=df_coupon_nume.columns,index=df_coupon_nume.index)\n",
    "        df_coupon_nume_norm=pd.DataFrame(ss.fit_transform(df_coupon_nume_im),columns=df_coupon_nume.columns,index=df_coupon_nume.index)\n",
    "        df_coupon_nume_norm=pd.DataFrame(mm.fit_transform(df_coupon_nume_norm),columns=df_coupon_nume.columns,index=df_coupon_nume.index)\n",
    "        ############################################\n",
    "        df_coupon_cate=df_coupon[['CAPSULE_TEXT', 'GENRE_NAME', 'large_area_name', 'ken_name','small_area_name']]\n",
    "        #sparse_m=binarizeCategorical(df_coupon_train_cat)\n",
    "        #sp.hstack(sparse_m,df_coupon_train_num.values)\n",
    "        df_coupon_binary=pd.DataFrame(self.binarizeCategorical(df_coupon_cate))  \n",
    "        df_coupon_combine=df_coupon_nume_norm.join(df_coupon_binary)\n",
    "        #df_coupon_combine.set_index('COUPON_ID_hash',inplace=True)\n",
    "        #return df_coupon_combine.as_matrix().astype(np.float)\n",
    "        return df_coupon_combine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#generate Xs, and persistentate the Xs,Xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19368, 163) (19368, 22782) (310, 163)\n"
     ]
    }
   ],
   "source": [
    "# make sure that the transformation is done to the whole \n",
    "fu_obj = FeatureUnion(transformer_list=[('item_feature', Get_coupon_feature())])\n",
    "train_part_coupon=df_coupon_train.set_index('COUPON_ID_hash').loc[df_MAP_coupon.COUPON_ID_hash].reset_index()\n",
    "\n",
    "Xs=fu_obj.fit_transform(pd.concat([train_part_coupon,df_coupon_test],axis=0)) # df_coupon_train is already the natural global index for items\n",
    "Xs_train=Xs[0:train_part_coupon.shape[0],:]\n",
    "Xs_test=Xs[train_part_coupon.shape[0]:,:]\n",
    "print Xs_train.shape,Xu_train.shape,Xs_test.shape\n",
    "with open('Xstr_Xutr_Xste.pickle', 'wb') as f:\n",
    "        pickle.dump((Xs_train,Xu_train,Xs_test),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#transform the hash code to natural index in df_coupon_train\n",
    "df_coupon_visit.drop(labels=[u'I_DATE',u'PAGE_SERIAL',u'REFERRER_hash',u'SESSION_ID_hash',u'PURCHASEID_hash'],axis=1,inplace=True)\n",
    "#g_visit=df_coupon_visit.groupby('VIEW_COUPON_ID_hash')\n",
    "#g_visit.count()\n",
    "g_visit2=df_coupon_visit.groupby(['VIEW_COUPON_ID_hash','USER_ID_hash'])\n",
    "#df_rating=pd.DataFrame(g_visit2.apply(lambda x: np.sum(x['PURCHASE_FLG'])))\n",
    "df_rating=df_coupon_visit[[u'USER_ID_hash',u'VIEW_COUPON_ID_hash','PURCHASE_FLG']] #duplicate coupon??????\n",
    "df_user['user_global_index']=df_user.index # all the users now has a unique index\n",
    "df_coupon_train['coupon_global_index']=df_coupon_train.index # all the coupons in training set now has a unique index\n",
    "#df_CouponProperty=pd.concat([df_coupon_train,df_coupon_test])\n",
    "# build up user global index and coupon global index for row and column index for sparse matrix\n",
    "user_id=df_user.set_index('USER_ID_hash').loc[df_rating['USER_ID_hash']]['user_global_index'].values\n",
    "coupon_id=df_coupon_train.set_index('COUPON_ID_hash').loc[df_rating['VIEW_COUPON_ID_hash']]['coupon_global_index'].values\n",
    "num_item,num_user=len(df_coupon_train.COUPON_ID_hash.unique()),len(df_user.USER_ID_hash.unique())\n",
    "V=df_rating.PURCHASE_FLG\n",
    "Xu=sparse.coo_matrix((V,(coupon_id,user_id)),shape=(num_item,num_user)) # the user_id looks like (1,1,1,2,2,3,4,4,...)\n",
    "#df_coupon_visit2=df_coupon_visit.groupby(['USER_ID_hash','VIEW_COUPON_ID_hash'])['PURCHASE_FLG'].agg({'PURCHASE_FLG_sum':'sum'})\n",
    "#df_rating=pd.DataFrame(g_visit2.agg(lambda x: np.sum(x['PURCHASE_FLG'])))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform the user-{NEW_item} rate matrix into submission form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_rt=pd.read_csv('user_item_rating.csv',header=None)\n",
    "df_rt=df_rt.set_index(df_MAP_user.USER_ID_hash)\n",
    "df_rt.columns=df_testsetMAP_Coupon.COUPON_ID_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_sub.set_index(userindex,inplace=True)\n",
    "def top_series(s, num):\n",
    "    tmp = pd.Series(s.order(ascending=False)[:num].index.tolist(), index = range(num))\n",
    "    return tmp\n",
    "dfdot_genre_top = df_rt.T.apply(lambda x: top_series(x, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final=dfdot_genre_top.T.apply((lambda x: \" \".join(x)), axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final=df_final.reset_index()\n",
    "\n",
    "df_final.columns=['USER_ID_hash','PURCHASED_COUPONS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([df_final,df_leakage],axis=0).to_csv('sub2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate user-coupon feature for FM/rf, but have memory problem, Xtrain is too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate feature for factorization machine user_[0,0,0,1,.....]_movie[1,0,0,0,0.....]_othermovies_[0.333,0.333,0.333,....][timeStamp]last_bought[0,1]-[target]\n",
    "\n",
    "#Generate training set\n",
    "df_coupon_transaction=df_coupon_visit# never modify the original one\n",
    "join_df = pd.merge(df_coupon_transaction, df_coupon_train,left_on=\"VIEW_COUPON_ID_hash\", right_on=\"COUPON_ID_hash\")\n",
    "join_df = pd.merge(join_df, df_user,left_on=\"USER_ID_hash\", right_on=\"USER_ID_hash\")\n",
    "feature_list = [('item_feature', Get_coupon_feature()),('user_item',Get_Match_Pref()),('getUserFeature',Get_user_feature())]\n",
    "fu_obj = FeatureUnion(transformer_list=feature_list)\n",
    "X_train = fu_obj.fit_transform(join_df)\n",
    "y_train = join_df[\"PURCHASE_FLG\"]\n",
    "assert X_train.shape[0] == y_train.size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate test set\n",
    "#generate the Cartesian Product\n",
    "df_coupon_test[\"cross\"] = 1 # all test items are new, ie, cold-start items\n",
    "df_user[\"cross\"] = 1 # all users are old\n",
    "test_df_item_user = pd.merge(df_coupon_test,df_user, on=\"cross\")# Cartesian product\n",
    "# create test Feature\n",
    "X_test = fu_obj.transform(test_df_item_user)\n",
    "# predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<ipython-input-1-3cdaee5bf3a1>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b762698850ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mpredict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mpos_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# find out which column is positive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtest_df_item_user\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"predict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/upb/departments/pc2/users/s/sunxd/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    511\u001b[0m                              \u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"threading\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_parallel_helper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'predict_proba'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m             for e in self.estimators_)\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;31m# Reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/upb/departments/pc2/users/s/sunxd/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    664\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/upb/departments/pc2/users/s/sunxd/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/upb/departments/pc2/users/s/sunxd/anaconda/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '<ipython-input-1-3cdaee5bf3a1>'"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "from sklearn import ensemble\n",
    "original_params = {'n_estimators': 1000,'n_jobs':-1}\n",
    "params = dict(original_params)\n",
    "clf=ensemble.RandomForestClassifier(**params)\n",
    "#clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "# create test_df\n",
    "\n",
    "#prediction\n",
    "\n",
    "\n",
    "predict_proba = clf.predict_proba(X_test)\n",
    "pos_idx = np.where(clf.classes_ == True)[0][0] # find out which column is positive\n",
    "test_df_item_user[\"predict\"] = predict_proba[:, pos_idx] \n",
    "\n",
    "top10_coupon=test_df_item_user.groupby(\"USER_ID_hash\").apply(extract_topn_item4user) #for each user\n",
    "top10_coupon.name = \"PURCHASED_COUPONS\"\n",
    "top10_coupon.to_csv(\"submission_rf.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# playground for sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import spdiags\n",
    "data = np.array([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])\n",
    "diags = np.array([0, -1, 2])\n",
    "data*data\n",
    "np.multiply(data,data)\n",
    "data,sp*data\n",
    "1. / (np.sqrt(np.sum(np.multiply(data,data), axis=1)))\n",
    "sp=sparse.spdiags(1./np.sqrt(np.sum(data * data, axis=1)),0,data.shape[0],data.shape[0])\n",
    "sp.todense()*data\n",
    "np.sum(data,axis=1)\n",
    "####################################\n",
    "sparse.coo_matrix.multiply(Xu,Xu.toarray())\n",
    "\n",
    "Xu.sum(1).shape\n",
    "(1. / (np.sqrt(Xu.multiply(Xu).sum(1)) )).shape\n",
    "1. / np.sqrt((Xu.multiply(Xu).sum(1)) ).T\n",
    "sparse.spdiags((1. / np.sqrt((Xu.multiply(Xu).sum(1)) )).T, 0, Xu.shape[0], Xu.shape[0]).todense()\n",
    "sparse.spdiags?\n",
    "\n",
    "row  = np.array([0, 3, 1, 0])\n",
    "col  = np.array([0, 3, 1, 2])\n",
    "data = np.array([4, 5, 7, 9])\n",
    "A=sparse.coo_matrix((data, (row, col)), shape=(4, 4))\n",
    "\n",
    "\n",
    "row = np.array([0, 2, 2, 0 ])\n",
    "col = np.array([0, 0, 1, 2])\n",
    "data = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "B=sparse.csc_matrix((data, (row, col)), shape=(4, 4))\n",
    "\n",
    "AA=sparse.csc_matrix(A)\n",
    "\n",
    "BB=sparse.csc_matrix(B)\n",
    "\n",
    "sparse.csc_matrix.multiply(AA,BB.T).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "BB.todense(),AA.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(BB+BB).todense(),BB.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.maximum(BB.toarray(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparse.csc_matrix.maximum(BB,1).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " print(\"Hu is %s\" %BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BB.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(BB.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_coupon_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Xs_test.pickle', 'wb') as f:\n",
    "    pickle.dump(Xs_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rating.cumsum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
